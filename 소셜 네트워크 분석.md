## 주제
네이버에 '코로나'를 검색했을 때 얻는 연관 검색어들과 
그 연관 검색어들 간의 네트워크를 통해 사람들의 '코로나'와 관련한 관심사에 대해 알아보고자 한다.



## 데이터 수집과정
먼저 '코로나'로 부터 10개의 연관 검색어를 크롤링한 다음 
얻게 된 연관 검색어들 부터 각각 10개씩 또다른 연관 검색어를 얻는 형태로 진행한다. (leaf)
6번 반복하여 데이터를 수집한다. (총 6개의 leaf)


![](D:/3-1/소셜네트워크/프로젝트/데이터수집.jpg)

![](D:/3-1/소셜네트워크/프로젝트/데이터수집total.jpg)

※ 중복되는 것들은 제외하고 결과(Result) 변수에 추가한다.




## library packages
필요한 패키지 library

```{r, warning=FALSE, message=FALSE}
library(httr)
library(dplyr)
library(rvest)
library(igraph)
library(readxl)
library(wordcloud2)
```


## COVID_19

먼저 검색창에 '코로나'(search_word)를 쳤을 때의 연관 검색어(related_keyword)들을 크롤링


※ 코드들은 실행 시간이 오래 걸리기 때문에 주석으로 표시
```{r }
# res <- GET(url = 'https://search.naver.com/search.naver?where=nexearch&query=%EC%BD%94%EB%A1%9C%EB%82%98&ie=utf8&sm=tab_she&qdt=0') # 검색창에 '코로나'를 검색했을 경우의 url을 GET하여 res객체에 넣어줌
# print(x=res)
# 
# related_keyword <- res%>%
#      read_html()%>%
#      html_nodes(css='#nx_related_keywords > dl > dd.lst_relate._related_keyword_list > ul > li:nth-child(n) > a')%>%
#      html_text(); related_keyword # 연관 검색어 부분을 크롤링하고 크롤링한 keyword를 related_keyword변수 안에 넣어줌
# 
# total_word <- as.vector(related_keyword); total_word # 검색어에 한번이라도 나온 변수
# total_word <- c(total_word,'코로나')
# 
# related_keyword <- data.frame(related_keyword) # 연관검색어 10개를 데이터 프레임 형으로 변환 
# search_word <- rep("코로나",times=10) # 검색어를 search_word변수 안에 넣어줌
# COVID_19 <- data.frame(search_word,related_keyword) # 데이터 프레임을 통해 하나로 합쳐줌
# 
# Result <- COVID_19 # 데이터를 계속 추가 할 변수

```




## COVID_19_leaf1

'코로나'의 연관 검색어를 검색어(search_word)로 하여 각각의 연관 검색어(related_keyword)들을 얻는다. 


```{r }
# leaf1_total <- c()
# leaf1_total <- as.vector(leaf1_total)
# 
# for(i in 1:nrow(related_keyword)) {
#   newdf <- related_keyword[i,] %>% URLencode() # url의 query부분에 넣어 줄 keyword를 퍼센트 인코딩함
#   linklist1 <- paste('https://search.naver.com/search.naver?where=nexearch&query=',newdf,'&ie=utf8&sm=tab_she&qdt=0',sep="") # url에서 검색어 부분을 newdf로 계속 바꿔줌
#   res1 <- GET(url = linklist1) # url을 GET하여 res1변수에 넣어 줌
#   
#   related_keyword1 <- res1%>%
#     read_html()%>%
#     html_nodes(css='#nx_related_keywords > dl > dd.lst_relate._related_keyword_list > ul > li:nth-child(n) > a')%>%
#     html_text() # 각각의 검색어에 대한 연관 검색어를 related_keyword1 변수에 저장
#   
#   related_keyword1 <- data.frame(related_keyword1) # 연관검색어를 데이터 프레임 형으로 변환
#   search_word1 <- rep(related_keyword[i,], times=10) # 해당 검색어를 search_word변수 안에 넣어줌
#   
#   COVID_19_leaf1 <- data.frame(search_word1,related_keyword1) # 데이터 프레임을 통해 하나로 합쳐줌
#   
#   names(COVID_19_leaf1) <- names(Result) # 열의 이름을 Result의 이름으로 바꿔줌
#   Result <- rbind(Result, COVID_19_leaf1) # 행으로 합침
#   
#   for(k in 1:10) {
#     if(related_keyword1[k,] %in% total_word) { # 이미 나온 검색어는 제외 시키고 추가함
#       next;
#     } else {
#       total_word <- c(total_word, related_keyword1[k,])
#       leaf1_total <- c(leaf1_total,related_keyword1[k,]) 
#     }
#   }
# }

```




## COVID_19_leaf2

위의 leaf1의 결과로 얻게된 연관 검색어(related_keyword)들을 검색어(search_word)로 하여 각각의 연관 검색어(related_keyword)들을 얻는다.


```{r }
# leaf2_total <- c()
# leaf2_total <- as.vector(leaf2_total)
# 
# for(i in 1:length(leaf1_total)) {
#   newdf <- leaf1_total[i] %>% URLencode() # url의 query부분에 넣어 줄 keyword를 퍼센트 인코딩함
#   linklist2 <- paste('https://search.naver.com/search.naver?where=nexearch&query=',newdf,'&ie=utf8&sm=tab_she&qdt=0',sep="") # url에서 검색어 부분을 newdf로 계속 바꿔줌
#   res2 <- GET(url = linklist2) # url을 GET하여 res2변수에 넣어 줌
#   
#   related_keyword2 <- res2%>%
#     read_html()%>%
#     html_nodes(css='#nx_related_keywords > dl > dd.lst_relate._related_keyword_list > ul > li:nth-child(n) > a')%>%
#     html_text() # 각각의 검색어에 대한 연관 검색어를 related_keyword2 변수에 저장
#   
#   related_keyword2 <- data.frame(related_keyword2) # 연관검색어를 데이터 프레임 형으로 변환
#   search_word2 <- rep(leaf1_total[i], times=nrow(related_keyword2)) # 해당 검색어를 search_word2변수 안에 넣어줌
#   
#   COVID_19_leaf2 <- data.frame(search_word2,related_keyword2) # 데이터 프레임을 통해 하나로 합쳐줌
#   
#   names(COVID_19_leaf2) <- names(Result) # 열의 이름을 Result의 이름으로 바꿔줌
#   Result <- rbind(Result, COVID_19_leaf2) # 행으로 합침
#   
#   for(k in 1:10) {
#     if(related_keyword2[k,] %in% total_word) { # 이미 나온 검색어는 제외 시키고 추가함
#       next;
#     } else {
#       total_word <- c(total_word, related_keyword2[k,])
#       leaf2_total <- c(leaf2_total,related_keyword2[k,])
#     }
#   }
# }

```




## COVID_19_leaf3

위의 leaf2의 결과로 얻게된 연관 검색어(related_keyword)들을 검색어(search_word)로 하여 각각의 연관 검색어(related_keyword)들을 얻는다.


```{r }
# leaf3_total <- c()
# leaf3_total <- as.vector(leaf3_total)
# 
# for(i in 1:length(leaf2_total)) {
#   newdf <- leaf2_total[i] %>% URLencode() # url의 query부분에 넣어 줄 keyword를 퍼센트 인코딩함
#   linklist2 <- paste('https://search.naver.com/search.naver?where=nexearch&query=',newdf,'&ie=utf8&sm=tab_she&qdt=0',sep="") # url에서 검색어 부분을 newdf로 계속 바꿔줌
#   res2 <- GET(url = linklist2) # url을 GET하여 res2변수에 넣어 줌
#   
#   related_keyword2 <- res2%>%
#     read_html()%>%
#     html_nodes(css='#nx_related_keywords > dl > dd.lst_relate._related_keyword_list > ul > li:nth-child(n) > a')%>%
#     html_text() # 각각의 검색어에 대한 연관 검색어를 related_keyword2 변수에 저장
#   
#   related_keyword2 <- data.frame(related_keyword2) # 연관검색어를 데이터 프레임 형으로 변환
#   search_word2 <- rep(leaf2_total[i], times=nrow(related_keyword2)) # 해당 검색어를 search_word2변수 안에 넣어줌
#   
#   COVID_19_leaf2 <- data.frame(search_word2,related_keyword2) # 데이터 프레임을 통해 하나로 합쳐줌
#   
#   names(COVID_19_leaf2) <- names(Result) # 열의 이름을 Result의 이름으로 바꿔줌
#   Result <- rbind(Result, COVID_19_leaf2) # 행으로 합침
#   
#   for(k in 1:10) {
#     if(related_keyword2[k,] %in% total_word) { # 이미 나온 검색어는 제외 시키고 추가함
#       next;
#     } else {
#       total_word <- c(total_word, related_keyword2[k,])
#       leaf3_total <- c(leaf3_total,related_keyword2[k,])
#     }
#   }
# }


```




## COVID_19_leaf4

위의 leaf3의 결과로 얻게된 연관 검색어(related_keyword)들을 검색어(search_word)로 하여 각각의 연관 검색어(related_keyword)들을 얻는다.


```{r }
# leaf4_total <- c()
# leaf4_total <- as.vector(leaf4_total)
# 
# for(i in 1:length(leaf3_total)) {
#   newdf <- leaf3_total[i] %>% URLencode() # url의 query부분에 넣어 줄 keyword를 퍼센트 인코딩함
#   linklist2 <- paste('https://search.naver.com/search.naver?where=nexearch&query=',newdf,'&ie=utf8&sm=tab_she&qdt=0',sep="") # url에서 검색어 부분을 newdf로 계속 바꿔줌
#   res2 <- GET(url = linklist2) # url을 GET하여 res2변수에 넣어 줌
#   
#   related_keyword2 <- res2%>%
#     read_html()%>%
#     html_nodes(css='#nx_related_keywords > dl > dd.lst_relate._related_keyword_list > ul > li:nth-child(n) > a')%>%
#     html_text() # 각각의 검색어에 대한 연관 검색어를 related_keyword2 변수에 저장
#   
#   related_keyword2 <- data.frame(related_keyword2) # 연관검색어를 데이터 프레임 형으로 변환
#   search_word2 <- rep(leaf3_total[i], times=nrow(related_keyword2)) # 해당 검색어를 search_word2변수 안에 넣어줌
#   
#   COVID_19_leaf2 <- data.frame(search_word2,related_keyword2) # 데이터 프레임을 통해 하나로 합쳐줌
#   
#   names(COVID_19_leaf2) <- names(Result) # 열의 이름을 Result의 이름으로 바꿔줌
#   Result <- rbind(Result, COVID_19_leaf2) # 행으로 합침
#   
#   for(k in 1:10) {
#     if(related_keyword2[k,] %in% total_word) { # 이미 나온 검색어는 제외 시키고 추가함
#       next;
#     } else {
#       total_word <- c(total_word, related_keyword2[k,])
#       leaf4_total <- c(leaf4_total,related_keyword2[k,])
#     }
#   }
# }

```




## COVID_19_leaf5

위의 leaf4의 결과로 얻게된 연관 검색어(related_keyword)들을 검색어(search_word)로 하여 각각의 연관 검색어(related_keyword)들을 얻는다.


```{r }
# leaf5_total <- c()
# leaf5_total <- as.vector(leaf5_total)
# 
# for(i in 1:length(leaf4_total)) {
#   newdf <- leaf4_total[i] %>% URLencode() # url의 query부분에 넣어 줄 keyword를 퍼센트 인코딩함
#   linklist2 <- paste('https://search.naver.com/search.naver?where=nexearch&query=',newdf,'&ie=utf8&sm=tab_she&qdt=0',sep="") # url에서 검색어 부분을 newdf로 계속 바꿔줌
#   res2 <- GET(url = linklist2) # url을 GET하여 res2변수에 넣어 줌
#   
#   related_keyword2 <- res2%>%
#     read_html()%>%
#     html_nodes(css='#nx_related_keywords > dl > dd.lst_relate._related_keyword_list > ul > li:nth-child(n) > a')%>%
#     html_text() # 각각의 검색어에 대한 연관 검색어를 related_keyword2 변수에 저장
#   
#   related_keyword2 <- data.frame(related_keyword2) # 연관검색어를 데이터 프레임 형으로 변환
#   search_word2 <- rep(leaf4_total[i], times=nrow(related_keyword2)) # 해당 검색어를 search_word2변수 안에 넣어줌
#   
#   COVID_19_leaf2 <- data.frame(search_word2,related_keyword2) # 데이터 프레임을 통해 하나로 합쳐줌
#   
#   names(COVID_19_leaf2) <- names(Result) # 열의 이름을 Result의 이름으로 바꿔줌
#   Result <- rbind(Result, COVID_19_leaf2) # 행으로 합침
#   
#   for(k in 1:10) {
#     if(related_keyword2[k,] %in% total_word) { # 이미 나온 검색어는 제외 시키고 추가함
#       next;
#     } else {
#       total_word <- c(total_word, related_keyword2[k,])
#       leaf5_total <- c(leaf5_total,related_keyword2[k,])
#     }
#   }
# }

```




## COVID_19_leaf6

위의 leaf5의 결과로 얻게된 연관 검색어(related_keyword)들을 검색어(search_word)로 하여 각각의 연관 검색어(related_keyword)들을 얻는다.


```{r }
# leaf6_total <- c()
# leaf6_total <- as.vector(leaf6_total)
# 
# for(i in 1:length(leaf5_total)) {
#   newdf <- leaf5_total[i] %>% URLencode() # url의 query부분에 넣어 줄 keyword를 퍼센트 인코딩함
#   linklist2 <- paste('https://search.naver.com/search.naver?where=nexearch&query=',newdf,'&ie=utf8&sm=tab_she&qdt=0',sep="") # url에서 검색어 부분을 newdf로 계속 바꿔줌
#   res2 <- GET(url = linklist2) # url을 GET하여 res2변수에 넣어 줌
#   
#   related_keyword2 <- res2%>%
#     read_html()%>%
#     html_nodes(css='#nx_related_keywords > dl > dd.lst_relate._related_keyword_list > ul > li:nth-child(n) > a')%>%
#     html_text() # 각각의 검색어에 대한 연관 검색어를 related_keyword2 변수에 저장
#   
#   related_keyword2 <- data.frame(related_keyword2) # 연관검색어를 데이터 프레임 형으로 변환
#   search_word2 <- rep(leaf5_total[i], times=nrow(related_keyword2)) # 해당 검색어를 search_word2변수 안에 넣어줌
#   
#   COVID_19_leaf2 <- data.frame(search_word2,related_keyword2) # 데이터 프레임을 통해 하나로 합쳐줌
#   
#   names(COVID_19_leaf2) <- names(Result) # 열의 이름을 Result의 이름으로 바꿔줌
#   Result <- rbind(Result, COVID_19_leaf2) # 행으로 합침
#   
#   for(k in 1:10) {
#     if(related_keyword2[k,] %in% total_word) { # 이미 나온 검색어는 제외 시키고 추가함
#       next;
#     } else {
#       total_word <- c(total_word, related_keyword2[k,])
#       leaf6_total <- c(leaf6_total,related_keyword2[k,])
#     }
#   }
# }


```
※ 코드들은 실행 시간이 오래 걸리기 때문에 주석으로 표시



## 수집된 데이터 결과 0525일자/ 0615일자
```{r }
#0525일자 결과
Result<-read.csv("D:/3-1/소셜네트워크/프로젝트/Result0525.csv")
Result<-Result[,c(2,3)]
str(Result)
head(Result,30)

```

```{r }
#0615일자 결과
leaf6<-read_xlsx("D:/3-1/소셜네트워크/프로젝트/leaf6.xlsx")
str(leaf6)
head(leaf6, 30)

```

모두 unique한 행들로 search_word, related_keyword의 두 열과 65022, 65665의 행으로 이루어져 있다.





## 횟수 별 네트워크
```{r }
leaf1<-read_xlsx("D:/3-1/소셜네트워크/프로젝트/leaf1.xlsx")
leaf2<-read_xlsx("D:/3-1/소셜네트워크/프로젝트/leaf2.xlsx")
leaf3<-read_xlsx("D:/3-1/소셜네트워크/프로젝트/leaf3.xlsx")
leaf4<-read_xlsx("D:/3-1/소셜네트워크/프로젝트/leaf4.xlsx")
leaf5<-read_xlsx("D:/3-1/소셜네트워크/프로젝트/leaf5.xlsx")

g1<-graph.data.frame(leaf1)
g2<-graph.data.frame(leaf2)
g3<-graph.data.frame(leaf3)
g4<-graph.data.frame(leaf4)
g5<-graph.data.frame(leaf5)
g6<-graph.data.frame(leaf6)


# 1번 반복
p1<-plot(g1,layout=layout_components,vertex.size=4,edge.arrow.size=0,vertex.label=V(g1)$name, 
     vertex.color = 'salmon',vertex.shape='square', vertex.label.dist=1,
     vertex.label.cex=0.9,vertex.label.color = "darkgreen")

# 2번 반복
p2<-plot(g2,layout=layout_components,vertex.size=3,edge.arrow.size=0,vertex.label=NA, 
         vertex.color = 'salmon',vertex.shape='square')

# 3번 반복
p3<-plot(g3,layout=layout_components,vertex.size=1.5,edge.arrow.size=0,vertex.label=NA, 
         vertex.color = 'salmon',vertex.shape='square')

# 4번 반복
p4<-plot(g4,layout=layout_components,vertex.size=1,edge.arrow.size=0,vertex.label=NA, 
         vertex.color = 'blue',vertex.shape='square')

# 5번 반복
p5<-plot(g5,layout=layout_with_fr,vertex.size=3,edge.arrow.size=0,vertex.label=NA, 
         vertex.color = 'violet',vertex.shape='square')

# 6번 반복
p6<-plot(g6,layout=layout_with_fr,vertex.size=3,edge.arrow.size=0,vertex.label=NA, 
         vertex.color = 'seagreen3',vertex.shape='square')


```


위의 1번 반복 네트워크를 보면 가운데 코로나 라는 단어를 중심으로 총 10개의 연관검색어 노드들이 있고, 그 노드들을 기준으로 각자의 연관검색어들이 서브그룹을 형성하는 것을 볼 수 있다. 또한 연관검색어들 사이에서도 선이 연결 되어있는 것으로 보아 연관검색어들 사이에서도 연관이 있는 것들이 존재하는 것을 알 수 있다.

4번 반복 데이터의 네트워크가 4개의 층을 형성하고 있는 것을 알 수 있다.



## 일자 별 네트워크
### 0525일자의 네트워크
```{r,Result, echo=FALSE}
g<-graph.data.frame(Result)
plot(g,layout=layout_with_fr,vertex.size=3,edge.arrow.size=0,vertex.label=NA, 
     vertex.color = 'salmon',vertex.shape='square')
```

### 0615일자의 네트워크
```{r,p6, echo=FALSE}
plot(g6,layout=layout_with_fr,vertex.size=3,edge.arrow.size=0,vertex.label=NA, 
         vertex.color = 'seagreen3',vertex.shape='square')
```


두 네트워크 비교를 통해 5월25일과 6월15일 사이에 연관검색어들의 변화가 있었음을 알 수 있다.



### 키워드 분석

코로나와 가장 연관성이 낮은 단어들을 보기 위해 6번 반복한 데이터에서 가장 마지막 반복후에 나온 연관검색어들만 뽑아서 워드클라우드를 그려본 결과이다. 이 워드클라우드를 통해 코로나라는 단어가 어떤 단어들까지 연결이 되어있는지에 대하여 알아볼 수 있다.


## 코로나와 낮은 연관성을 가지는 키워드 - 5/25데이터
```{r message=FALSE}
Leaf6_525 <- read_xlsx("D:/3-1/소셜네트워크/프로젝트/Leaf6_525.xlsx")

Leaf6_525 <- Leaf6_525%>%
  group_by(leaf6_total)%>%
  summarise(count=n())%>%
  arrange(desc(count))
Leaf6_525 <- data.frame(Leaf6_525)

wordcloud2::wordcloud2(Leaf6_525, size=0.2,color = "random-light", backgroundColor = "white", fontFamily = "나눔바른고딕")
```


5월 25일 워드클라우드를 보면 '새우감바스만들기, 인천 드라이브 코스, 1박2일 여행지, 1박2일 속초여행' 등 일상/문화와 관련된 단어들 그리고 '(주)'라는 글이 붙어있는 상장회사들의 이름이 많은 것을 볼 수 있다. 이것을 통해 사람들이 어떤 일상/문화에 관심이 많은지 그리고 주식과 관련해서 관심이 많다는 사실을 알 수 있다.



## 코로나와 낮은 연관성을 가지는 키워드 - 6/15데이터
```{r message=FALSE}
# Leaf6_615 <- read_xlsx("D:/3-1/소셜네트워크/프로젝트/Leaf6_615.xlsx")
# 
# Leaf6_615 <- Leaf6_615%>%
#   group_by(related_keyword)%>%
#   summarise(count=n())%>%
#   arrange(desc(count))
# Leaf6_615 <- data.frame(Leaf6_615)
# 
# wordcloud2::wordcloud2(Leaf6_615, size=0.2,color = "random-light", backgroundColor = "white", fontFamily = "나눔바른고딕")
```
![](D:/3-1/소셜네트워크/프로젝트/0615_낮은연관성.png)

6월15일 워드클라우드를 살펴보면 이때도 '서울이색데이터, 2021예언, 11월 삿포르여행' 등 많은 일상/문화 검색어들을 살펴볼 수 있고, 5월25일 워드클라우드와는 다르게 상장회사들의 이름보다는 '금 시세, 금, 18K' 등 금과 관련된 단어들이 많은 것으로 보아 사람들이 금에 관심이 많다는 사실을 알 수 있다.



## 코로나와 높은 연관성을 가지는 키워드 - 5/25데이터
```{r message=FALSE}
Result_525 <- read_xlsx("D:/3-1/소셜네트워크/프로젝트/Result_525.xlsx") #5월25일에 수집된 데이터를 불러옴.

bindow_0525 <- Result_525%>% #related_keyword별 빈도수를 내림차순으로 정렬하여 상위150개만 추출
  group_by(related_keyword)%>%
  summarise(count=n())%>%
  arrange(desc(count))%>%
  head(150)
bindow_0525 <- data.frame(bindow_0525)
# 
# wordcloud2::wordcloud2(bindow_0525,color = "random-light", size=0.35, backgroundColor = "white",fontFamily = '나눔바른고딕') #워드클라우드 생성
```
![](D:/3-1/소셜네트워크/프로젝트/0525_높은연관성.png)

다음과 같이 생성된 5월 25일 워드클라우드를 보면 '코로나, 코로나19, 코로나증상, 코로나 확진자'와 같은 단어들을 중심으로 그 주위에 코로나 증상과 관련된 단어들이 많은 것을 확인할 수 있다. 또한, '파미셀, 삼성전자, 씨젠'과 같이 상장기업들의 이름이 많이 있는 것으로 보아 낮은 키워드 워드클라우드에서 알 수 있었던 사람들이 주식에 관심이 있다는 정보에 더하여  주식 중에서도 백신을 개발하는 기업들의 주식에 관심이 있다는 사실을 추가로 알 수 있다.


다음으로 연관 검색어의 degree를 내림차순한 결과이다.
```{r, Result_525}
R_515 <- graph.data.frame(Result_525, directed = F)
head(sort(degree(R_515),decreasing = T),70)
```


코로나의 증상이나 확진자, 발생 장소, 개학연기 등과 관련된 단어들이 연관 검색어로 많이 등장하는 것을 알수 있으며, 이 밖에도 5월의 코로나와 높은 연관성을 가지는 키워드를 확인할 수 있다.



## 코로나와 높은 연관성을 가지는 키워드 - 6/15데이터
```{r message=FALSE}
Result_615 <- read_xlsx("D:/3-1/소셜네트워크/프로젝트/Result_615.xlsx") #6월15일에 수집된 데이터를 불러옴.

bindow_0615 <- Result_615%>% #related_keyword별 빈도수를 내림차순으로 정렬하여 상위150개만 추출
  group_by(related_keyword)%>%
  summarise(count=n())%>%
  arrange(desc(count))%>%
  head(150)
bindow_0615 <- data.frame(bindow_0615)
# 
# wordcloud2::wordcloud2(bindow_0615,color = "random-light", size=0.35, backgroundColor = "white",fontFamily = '나눔바른고딕') #워드클라우드 생성
```
![](D:/3-1/소셜네트워크/프로젝트/0615_높은연관성.png)

6월15일 워드클라우드를 보면 5월25일 워드클라우드와 마찬가지로 '코로나, 코로나19, 코로나 증상, 코로나 19'와 같은 단어들이 중심에 있는 것을 확인할 수 있다. 또한 그 주위에 '코스피, 삼성전자' 등의 단어가 나타나 있는 것을 통해 주식과 관련된 관심이 여전히 진행 중임을 알 수 있다. 그 외에도 '학생건강상태자가진단, 학생코로나 자가 진단' 등 등교를 시작한 초/중/고등학교와 관련된 단어들이 많이 나타나 있는 것을 볼 수 있다.


다음으로 연관검색어의 degree를 내림차순한 결과이다.
```{r, Result_615}
R_615 <- graph.data.frame(Result_615, directed = F)
head(sort(degree(R_615),decreasing = T),70)
```


5월과 마찬가지로 코로나와 증상, 확진자에 대한 연관검색어가 여전히 많이 등장하는 한편, 삼성전자나 코스피에 관련된 연관검색어가 새롭게 등장하거나 6월에 더 많이 등장하는 것을 알 수 있다. 그 밖에도 6월의 코로나와 높은 연관성을 가지는 키워드를 확인할 수 있다.




### 랜덤 그래프, SF그래프와 비교
```{r fig.show='hold'}
g0525<-graph.data.frame(Result)
g0615<-graph.data.frame(leaf6)


## 0525
nrow(Result)
length(V(g0525))
length(E(g0525))
edge_density(g0525)
mean(degree(g0525))
diameter(g0525)


## 0615
nrow(leaf6)
length(V(g0615))
length(E(g0615))
edge_density(g0615)
mean(degree(g0615))
diameter(g0615)


## random
# 0525, 0615 두 그래프의 노드와 edge수의 평균값으로 구함
n<-(length(V(g0525))+length(V(g0615))+1)/2
p<-(length(E(g0525))+length(E(g0615))/2)/(n*(n-1)/2)

g_r<-erdos.renyi.game(n,p)
length(V(g_r))
length(E(g_r))
edge_density(g_r)
mean(degree(g_r))
diameter(g_r) #8



# SF네트워크
g_b<-barabasi.game(n,out.pre=n)
length(V(g_b))
length(E(g_b))
edge_density(g_b)
mean(degree(g_b))
diameter(g_b) #17



#hist 비교
par(mfrow=c(2,2))
hist(degree(g0525))
hist(degree(g0615))
hist(degree(g_r))
hist(degree(g_b))

```

| / | 0525 network | 0615 network | Random network | SF network |
| --- | :---: | :---: | :---: | :---: |
| Node | 22477 | 22098 | 22288 | 22288 |
| Edge | 65022 | 65665 | 97629 | 22287 |
| Edge Density | 0.0001287072 | 0.0001344769 | 0.0003930847 | 4.486719e-05 |
| Mean Degree | 5.785648 | 5.943072 | 8.760678 | 1.99991 |
| Diameter | 19 | 21 | 8 | 17 |


5월25일 네트워크, 6월15일네트워크, 랜덤네트워크, SF네트워크를 비교한 결과이다. 히스토그램의 위에 왼쪽,오른쪽이 각각 5월25일, 6월15일 네트워크의 히스토그램, 아래 왼쪽오른쪽은 각각 랜덤네트워크의 degree히스토그램, SF네트워크의 히스토그램이다.
히스토그램만 보면 5월,6월 네트워크가 SF그래프와 유사해 보이는데 mean degree값을 비교해보면 랜덤네트워크에 좀더 가까운 것을 알 수 있다.



### 결론
1. 확진자 수/코로나 발병일 증가여부와 상관없이 코로나19상황, 코로나 확진자 동향, 코로나증상에는 항상 높은 관심을 보임

2. 확진자 수의 증가와 상관없이 코로나 발병일이 길어질수록 일상/여가문화와 관련된 검색어의 빈도가 높아짐

3. 확진자 수의 변화와 코로나 발병일의 변화에 따라 코로나로 시작해서 연결되는 검색어들(네트워크)이 달라짐

4. 키워드를 홍보 마케팅에 이용하고자 한다면 코로나 관련 키워드들과 일상/여가문화와 관련된 키워드가 좋을 것이라고 생각됨 

5. 그 밖에도 코로나와 관련하여 백신에 투자하는 여러 주식회사들이 종종 연관 검색어에 등장하는 것을 알 수 있었음


※ 참고 
데이터 수집 당시 확진자 수
5/25 : 16명 (해외3 국내13)
6/15 : 37명 (해외13 국내24)











